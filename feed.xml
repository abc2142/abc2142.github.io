<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://abc2142.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://abc2142.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-13T04:17:41+00:00</updated><id>https://abc2142.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Einsum in 7 minutes</title><link href="https://abc2142.github.io/blog/2025/einsum/" rel="alternate" type="text/html" title="Einsum in 7 minutes"/><published>2025-08-12T00:00:00+00:00</published><updated>2025-08-12T00:00:00+00:00</updated><id>https://abc2142.github.io/blog/2025/einsum</id><content type="html" xml:base="https://abc2142.github.io/blog/2025/einsum/"><![CDATA[<p>Einsum can make multi-dimensional linear algebraic array operations simple, with unified syntax. It is simpler to describe it using math than words. I will give a few examples of its usage in PyTorch.</p> <p><strong>Notation</strong>:</p> <ol> <li>The entry in row $i$, column $j$ of matrix $A$ is denoted by $A_{i,j}$.</li> <li>An entry in a three diemensional tensor $X$ will be similarly denoted $X_{b, i, j}$.</li> <li>$X_{b, :, :}$ is a matrix corresponding to the batch element $b$ of the 3D tensor $X$.</li> </ol> <p>We have matrix $X$ and $Y$, and want to implement $Z=XY$, where</p> <p>\begin{equation} Z_{i,k} = \sum_{j} X_{i,j} Y_{j,k} \end{equation}</p> <p>In PyTorch, this can be implemented as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ij, jk -&gt; ik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</code></pre></div></div> <p>Now, suppose that $X$ and $Y$ represent 3-D tensors, where the first dimension represents a batch dimension, and we want to perform matrix multiplication over the last two dimenensions, for each batch element. \begin{equation} Z_{b, i,k} = \sum_{j} X_{b, i,j} Y_{b, j,k} \end{equation}</p> <p>In PyTorch, this can be implemented as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</code></pre></div></div> <p>In the final example, we Gram matrix computation $G_{b,:,:}=X_{b,:,:}^T X_{b,:,:}$ over the last two dimensions of 3-D array $X$. For each batch element $b$, we have a matrix $X_{b,:,:}$, and compute its Gram matrix \begin{equation} G_{b,i,j} = \sum_j X_{b,j,i} X_{b,j,k} \end{equation}</p> <p>So, we can implement either</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bji, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <p>or</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bij, bjk -&gt; bik</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <h2 id="jupyter-notebook">Jupyter Notebook</h2> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/einsum.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="PyTorch"/><summary type="html"><![CDATA[Einsum tutorial]]></summary></entry></feed>